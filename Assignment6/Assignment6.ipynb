{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 asked us to fit a Poisson Regression GLM to the crime data in Alameda county using the weather data. To accomplish this task, we used some of the functions we had written in previous assignments. Additionally, it takes awhile for some of the data to be computed, so we saved some of it in pickle files and reused them when possible. We also display the original code used to get the data. The first thing we needed to do was get the weather data for East Alameda and West Alameda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alameda_county_points():\n",
    "    \"\"\" Calculates all the grid points (lat, lon) inside Alameda County\n",
    "    Grid automatically includes Summit Reservoir (37.905098, -122.272225)\n",
    "    Begin at northwest corner of bounding box and move in increment of 5 miles\n",
    "    in the south and east directions until we reach the southeast corner,\n",
    "    recording all grid points that fall inside Alameda County.\n",
    "\n",
    "    return:\n",
    "        list: list of (lat, lon) points inside Alameda County\n",
    "    \"\"\"\n",
    "    # bounding box around Alameda County\n",
    "    north = 38\n",
    "    west = -122.4\n",
    "    south = 37.4\n",
    "    east = -121.4\n",
    "\n",
    "    # grid automatically includes Summit Reservoir (37.905098, -122.272225)\n",
    "    grid_points = [(37.905098, -122.272225)]\n",
    "    curr = [north, west]\n",
    "\n",
    "    # while current point is within the north / south bounds\n",
    "    while curr[0] > south:\n",
    "        # dynamically update lat and lon increment based on curr point\n",
    "        destE = vincenty(miles=5).destination(curr, 90) # point 5 miles east of curr\n",
    "        lon_increment = destE.longitude - curr[1]\n",
    "        destS = vincenty(miles=5).destination(curr, 180) # point 5 miles south of curr\n",
    "        lat_increment = curr[0] - destS.latitude\n",
    "\n",
    "        # while current point is within the east / west bounds\n",
    "        while curr[1] < east:\n",
    "            if (rg.search(curr)[0]['admin2'] == \"Alameda County\"):\n",
    "                grid_points.append(tuple(curr))\n",
    "            curr[1] += lon_increment\n",
    "        curr[0] -= lat_increment\n",
    "        curr[1] = west\n",
    "\n",
    "    return grid_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_alameda_points():\n",
    "    \"\"\" Return a list of West Alameda points and a list of East Alameda points\n",
    "\n",
    "    return:\n",
    "        list: list of West Alameda (lat, lon) points\n",
    "        list: list of East Alameda (lat, lon) points\n",
    "    \"\"\"\n",
    "    grid_points = get_alameda_county_points()\n",
    "    border_west = -122.06 # westernmost point of \"East Alameda\", points to the left are in West Alameda\n",
    "    border_east = -121.82 # easternmost point in \"West Alameda\", points to the right are in East Alameda\n",
    "\n",
    "    # zipcodes identified to be in West or East Alameda based on Google Maps\n",
    "    zip_loc = {\n",
    "        'west': [94539, 94552, 94544, 94536, 94537, 94538],\n",
    "        'east': [94568, 94566, 94586]\n",
    "    }\n",
    "\n",
    "    west_alameda = []\n",
    "    east_alameda = []\n",
    "    for point in grid_points:\n",
    "        if point[1] < border_west:\n",
    "            west_alameda.append(point)\n",
    "        elif point[1] > border_east:\n",
    "            east_alameda.append(point)\n",
    "        else:\n",
    "            # classify ambiguous points that lie between border_west and border_east\n",
    "            try:\n",
    "                # get zipcode of unknown point\n",
    "                unknown_zip = int(re.findall(r'\\d+', geolocator.reverse(point).raw['address']['postcode'])[0])\n",
    "                if unknown_zip in zip_loc['west']:\n",
    "                    west_alameda.append(point)\n",
    "                elif unknown_zip in zip_loc['east']:\n",
    "                    east_alameda.append(point)\n",
    "            except:\n",
    "                # Some points represent landmarks or roads that stretch multiple zipcodes, \n",
    "                # causing the above code to error. We handle these cases separately.\n",
    "                unknown_loc = geolocator.reverse(point).raw['address']['hamlet']\n",
    "                if unknown_loc == 'Kilkare Woods':\n",
    "                    west_alameda.append(point)\n",
    "            \n",
    "    return west_alameda, east_alameda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrected_weather_data(load_pickle=False):\n",
    "    \"\"\" Get corrected weather data for the areas of interest\n",
    "\n",
    "    return\n",
    "        DataFrame: df from west Alameda\n",
    "        DataFrame: df from east Alameda\n",
    "        DataFrame: df from Alameda\n",
    "    \"\"\"\n",
    "    w_pts, e_pts = divide_alameda_points()\n",
    "\n",
    "    alameda = pickle.load(open('Assignment6/alameda_weather', 'rb'))\n",
    "    east_weather = pickle.load(open('Assignment6/east_weather_file', 'rb'))\n",
    "    west_weather = pickle.load(open('Assignment6/west_weather_file', 'rb'))\n",
    "\n",
    "    # load from assignment 3 function\n",
    "    if not load_pickle:\n",
    "        alameda = main(elements=['TMAX', 'PRCP'])\n",
    "        east_weather = main(points = e_pts, elements=['TMAX', 'PRCP'])\n",
    "        west_weather = main(points = w_pts, elements=['TMAX', 'PRCP'])\n",
    "    return west_weather, east_weather, alameda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pull the data from the `main` function in assignment 3. It essentially takes a list of points, identifies weather stations within a particular radius of those points, and then it creates a data frame for each year and month combinations that bins the inverse weighted average temperature from the weather stations. The temperature reported is corrected for bias. After we do this, we need to get the crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_data(load_pickle=False):\n",
    "    \"\"\" Creates a DataFrame of crime data for Alameda County from 1980 to 2009\n",
    "        Pickle this dataframe for future use\n",
    "\n",
    "    return:\n",
    "        DataFrame: Crime data DF\n",
    "    \"\"\"\n",
    "    if load_pickle:\n",
    "        return pickle.load(open('Assignment6/df_1980_2009.pkl', 'rb'))\n",
    "    # get list of dta files for 1980 - 2009 data\n",
    "    crime_data_path = \"Assignment6/data/ucr_offenses_known_monthly_1960_2016_dta/\"\n",
    "    dta_files = [f for f in os.listdir(crime_data_path) if f[-4:] == '.dta']\n",
    "    dta_files_80_09 = [f for f in dta_files if int(f[-8:-4]) >= 1980 and int(f[-8:-4]) <= 2009]\n",
    "\n",
    "    # 1980 - 2009 crime data\n",
    "    df_list = []\n",
    "    for f in dta_files_80_09:\n",
    "        df = pd.read_stata(crime_data_path + f)\n",
    "        # get only Alameda County data point\n",
    "        df = df[(df['fips_state_code'] == '06') & (df['fips_county_code'] == '001')]\n",
    "        df_list.append(df)\n",
    "    df_80_09 = pd.concat(df_list)\n",
    "    df_80_09.to_pickle(\"Assignment6/data/df_1980_2009.pkl\")\n",
    "    return df_80_09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the crime data by concatenating all the relevant years and then pickling the data. Once again, we pickle it because this operation takes a really long time. The next step for us is to clean the crime data. We needed to put it into a form that can be easily joined with the weather data to create our design matrix for the Poisson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crime_data(alameda_crime):\n",
    "    \"\"\" Take crime data, clean it, and divide it into the areas of interest\n",
    "    \n",
    "    args:\n",
    "        DataFrame: crime data DF\n",
    "    return:\n",
    "        DataFrame: df from west Alameda\n",
    "        DataFrame: df from east Alameda\n",
    "        DataFrame: df from Alameda\n",
    "    \"\"\"\n",
    "    # get sum of crime incidences\n",
    "    alameda_crime['total_crime'] = alameda_crime.filter(regex='_total').sum(axis=1)\n",
    "    east = alameda_crime[alameda_crime['zip_code'].isin(classify_zip['east'])]\n",
    "    west = alameda_crime[alameda_crime['zip_code'].isin(classify_zip['west'])]\n",
    "    \n",
    "    # get cleaned data\n",
    "    east_cleaned = east.groupby(['year', 'month']).sum()[['total_crime']].reset_index()\n",
    "    west_cleaned = west.groupby(['year', 'month']).sum()[['total_crime']].reset_index()\n",
    "    alameda_cleaned = alameda_crime.groupby(['year', 'month']).sum()[['total_crime']].reset_index()\n",
    "\n",
    "    # change month name to month number\n",
    "    east_cleaned['month'] = pd.to_datetime(east_cleaned['month'], format='%B').dt.month\n",
    "    west_cleaned['month'] = pd.to_datetime(west_cleaned['month'], format='%B').dt.month\n",
    "    alameda_cleaned['month'] = pd.to_datetime(alameda_cleaned['month'], format='%B').dt.month\n",
    "\n",
    "    return west_cleaned, east_cleaned, alameda_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the total crime in Alameda. Then, it divides the county into East and West. Next, we group the data by month and year combination and then sum it. Additionally, we need to convert the month strings into month indexes so that it is easy for us to merge it with our weather data frame. Our next step is to merge the two and data frames to create our data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_matrix(weather, crime):\n",
    "    \"\"\" Gets the regression data matrix from the weather and crime data\n",
    "\n",
    "    args:\n",
    "        weather: weather DataFrame\n",
    "        crime: crime DataFrame\n",
    "    return:\n",
    "        ndarray: design matrix\n",
    "        ndarray: dependent variable\n",
    "    \"\"\"\n",
    "    # merge temperature with precipitation\n",
    "    a_temp, a_prcp = weather\n",
    "    a_temp.columns = a_temp.columns.astype(str)\n",
    "    a_prcp.columns = a_prcp.columns.astype(str)\n",
    "    merged_weather = a_temp.join(a_prcp)\n",
    "\n",
    "    # merge crime with weather\n",
    "    merged = pd.merge(crime, merged_weather.reset_index(), on=['year', 'month'], how='inner').fillna(0.0) \n",
    "    merged_data = merged.sort_values(by=['year', 'month']).reset_index(drop=True).drop(['year', 'month'], axis=1)\n",
    "\n",
    "    # get x and y values\n",
    "    y = merged_data['total_crime']\n",
    "    x = merged_data.drop('total_crime', axis=1).values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can take the weather data and crime data and combine it form our X and Y data. We merge the crime data with the weather data and then pull the dependent variable and the covariate values out. Finally, using the data we need to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x, y):\n",
    "    \"\"\" Gets fitted GLM Poisson regression from x and y\n",
    "\n",
    "    args:\n",
    "        x: design matrix\n",
    "        y: dependent variable\n",
    "    return:\n",
    "        GLM: model\n",
    "    \"\"\"\n",
    "    return sm.GLM(y, x, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply takes in an X and a Y and returns the fitted Poisson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_assignment6():\n",
    "    \"\"\" Run assignment 6 to get the models\n",
    "\n",
    "    return:\n",
    "        GLM: west\n",
    "        GLM: east\n",
    "        GLM: alameda\n",
    "    \"\"\"\n",
    "    west, east, alameda = get_corrected_weather_data(load_pickle=True)\n",
    "    crime_data = get_crime_data(load_pickle=True)\n",
    "    west_crime, east_crime, alameda_crime = clean_crime_data(crime_data)\n",
    "\n",
    "    west_x, west_y = get_data_matrix(west, west_crime)\n",
    "    east_x, east_y = get_data_matrix(east, east_crime)\n",
    "    alameda_x, alameda_y = get_data_matrix(alameda, alameda_crime)\n",
    "\n",
    "    return get_model(west_x, west_y), get_model(east_x, east_y), get_model(alameda_x, alameda_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
