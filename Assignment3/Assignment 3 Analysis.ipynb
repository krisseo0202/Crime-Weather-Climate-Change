{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../Assignment4/')\n",
    "sys.path.insert(0, '../Assignment3/')\n",
    "sys.path.insert(0, '../Assignment2/')\n",
    "sys.path.insert(0, '../Assignment1/')\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from assignment2 import get_stations, get_alameda_county_points, get_station_weights\n",
    "from weighted_avg import calc_inv_weighted_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we had to do for Assignment 3 is to pull all the stations within 10 miles of any grid point in Alameda County and then get all the weather data for the relevant time periods from those stations. We noticed that we can find out which station weather data came from based on the ID column in the weather data and station data. Once we figured this out, we pulled both datasets into pandas DataFrames and merged them on the ID of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "def get_weather_data(points = get_alameda_county_points(), max_distance = 10):\n",
    "    \"\"\"Gets weather data for a given set of grid points\n",
    "\n",
    "    args:\n",
    "        points: list of grid points (lat, lon) (default alameda county grid)\n",
    "        max_distance: max distance to search around each grid point (default 10)\n",
    "    return:\n",
    "        pd.DataFrame: dataframe with data from weather and stations\n",
    "    \"\"\"\n",
    "\n",
    "    # load data from csv\n",
    "    stations = pd.DataFrame(get_stations(points, max_distance))\n",
    "    weather = pd.read_csv('Assignment3/data/ca_weather_data.csv')\n",
    "\n",
    "    # merge data on the ID\n",
    "    merged = stations.merge(weather, on='ID', how='inner')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we retrieved the data for the relevant time periods, we had to limit our search to meet Ranson's criteria for inclusion in each year. However, to do this, we needed to clean our data so that we would be able to access the year attribute. Additionally, we need some properties like year, month, and day that we used later in the analysis. To speed up computation, we removed all rows that weren't pertinent to our analysis (eg. PRCP data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\" Cleans data from original dataframe\n",
    "\n",
    "    args:\n",
    "        df: dataframe of data\n",
    "    return:\n",
    "        df: cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # clean the date data\n",
    "    df['date'] = pd.to_datetime(df['YEARMONTHDAY'].astype(str), format='%Y%m%d')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    # only keep temperature rows\n",
    "    df = df[df['ELEMENT'] != 'PRCP']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranson's criteria for inclusion was to only include data points for a particular (weather station, year, element) tuple that had atleast 100 records that matched it. To do this, we counted all the records in the data set grouped by the (weather station, year, element). Once we had this data, we filtered the rows to include only (weather station, year, element) tuples that had atleast 100 records. With this new dataset, we were able to get the intersection of the new dataset and our original dataset to get the records we wanted. We accomplished this by doing an inner join on the tables on the (weather station, year, element) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ranson_criteria(df):\n",
    "    \"\"\"Filters the rows based on Ranson's criteria which states\n",
    "    to only include year, station, element combinations that have\n",
    "    more than 100 points\n",
    "    \n",
    "    args:\n",
    "        df: dataframe of data\n",
    "    return:\n",
    "        df: dataframe of data with only ranson criteria satisfying rows\n",
    "    \"\"\"\n",
    "    \n",
    "    yearly_count = df.groupby(['NAME','year', 'ELEMENT']).count()\n",
    "    new_df = yearly_count.reset_index()\n",
    "    new_df = new_df[new_df['ID'] >= 100][['NAME', 'year', 'ELEMENT']]\n",
    "    return new_df.merge(df, how='inner', on=['NAME', 'year', 'ELEMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we had to run our bias correction algorithm. The bias correction algorithm was given to us in Lab 8. We implemented this by first initializing all of our parameters. Then we ran a while loop that would continue until the convergence criteria is met. The convergence criteria will be discussed later in this description.\n",
    "\n",
    "We first identify a randomly selected reference station. Next, we follow the algorithm to get all the data where the stations intersect and sum them in the way the equation mentions to. We calculate the station intersection by filtering the original dataframe by the station name and then merging the two datasets on the date. We use an inner join so that we only get rows that intersect. In this way, we are able to determine all the data points from both the station and the reference. This data along with the number of intersected rows allows us to update the intercept value for that station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_intersection(station1_name, station2_name, df):\n",
    "    \"\"\"Gets the intersection of rows between station1 and station2\n",
    "\n",
    "    args:\n",
    "        station1_name: name of station 1\n",
    "        station2_name: name of station 2\n",
    "        df: dataframe of data\n",
    "    return:\n",
    "        int: length of the merged data\n",
    "        df: dataframe of intersected data\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the days that the stations intersect\n",
    "    station1_rows = df[df['NAME'] == station1_name]\n",
    "    station2_rows = df[df['NAME'] == station2_name]\n",
    "    merged = station1_rows.merge(station2_rows, on='date', how='inner')\n",
    "    return len(merged['date']), merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this procedure till convergence. Our convergence criteria was to ensure that every (2 * # of stations) time we run the algorithm, the difference between the original intercepts calculated aren't 'too different' from the iteration (2 * # of stations) before the current iteration. We define 'too different' as having a loss less than 0.000001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction(df):\n",
    "    \"\"\"Adjusts the data by doing a bias correction\n",
    "\n",
    "    args:\n",
    "        df: dataframe to run bias correction on\n",
    "    return:\n",
    "        df: dataframe with bias corrected data\n",
    "    \"\"\"\n",
    "\n",
    "    stations = df['NAME'].unique()\n",
    "\n",
    "    # initialize intercepts\n",
    "    intercepts = [0 for _ in range(len(stations))]\n",
    "    old_intercepts = intercepts[:]\n",
    "    count = 0\n",
    "\n",
    "    # iterate till convergence\n",
    "    while True:\n",
    "\n",
    "        # pick random point in reference\n",
    "        reference_idx = np.random.randint(0, len(stations))\n",
    "        reference_station = stations[reference_idx]\n",
    "\n",
    "        # iterate through all stations to update intercept\n",
    "        for station_idx in range(len(stations)):\n",
    "            station = stations[station_idx]\n",
    "\n",
    "            # get intersection of stations\n",
    "            n, k = station_intersection(reference_station, station, df)\n",
    "\n",
    "            # calculate sum for all intersected rows\n",
    "            curr_sum = 0.0\n",
    "            for _, row in k.iterrows():\n",
    "                station1_data = row['DATA VALUE_x']\n",
    "                station2_data = row['DATA VALUE_y']\n",
    "                curr_sum += station1_data + intercepts[reference_idx] - (station2_data + intercepts[station_idx])\n",
    "\n",
    "            # update the stations intercept\n",
    "            if n != 0:\n",
    "                intercepts[station_idx] = intercepts[station_idx] + curr_sum / n\n",
    "\n",
    "        if count % (2 * len(stations)) == 0 and count != 0:\n",
    "            # check if it converges by calculating loss\n",
    "            loss = np.sum((np.array(intercepts) - np.array(old_intercepts)) ** 2)\n",
    "            old_intercepts = intercepts[:]\n",
    "            # set convergence criteria\n",
    "            if loss < 0.000001:\n",
    "                break\n",
    "        count += 1\n",
    "\n",
    "    # map station name to intercept value\n",
    "    bias = dict(zip(stations, intercepts))\n",
    "\n",
    "    # get latitude and longitude of stations\n",
    "    lat_lon = []\n",
    "    for station in stations:\n",
    "        data = df[df['NAME'] == station].iloc[0]\n",
    "        latitude, longitude = data['LATITUDE'], data['LONGITUDE']\n",
    "        lat_lon.append((latitude, longitude))\n",
    "\n",
    "    # get station weights\n",
    "    weights = np.array(calc_inv_weighted_avg(get_alameda_county_points(), lat_lon))\n",
    "    weights = weights * np.array(intercepts) / np.sum(weights)\n",
    "\n",
    "    # calculate C\n",
    "    C = dict(zip(stations, weights))\n",
    "\n",
    "    # apply formula to calculate adjusted data\n",
    "    df['adjusted_data'] = df.apply(lambda x: x['DATA VALUE'] + bias[x['NAME']] - C[x['NAME']], axis = 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this algorithm is to bin the data. With some help from stackoverflow, we were able to find an easy way to do this. Essentially, we ran our entire algorithms through the above functions and grouped the data set by year and month and then binned the bias corrected data into bins [0, 300, 10] for each of the elements we were interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(elements=['TMAX', 'TMIN'], points = get_alameda_county_points()):\n",
    "    \"\"\"Bins adjusted data for each element. Binning idea received\n",
    "    by: https://stackoverflow.com/questions/34317149/pandas-groupby-with-bin-counts\n",
    "    \n",
    "    args:\n",
    "        elements: specifies elements to calculate data for (default ['TMAX', 'TMIN'])\n",
    "    return:\n",
    "        list: returns list of binned data as dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = filter_ranson_criteria(clean_data(get_weather_data(points)))\n",
    "    result = []\n",
    "    for element in elements:\n",
    "        filtered = df[df['ELEMENT'] == element]\n",
    "        corrected = bias_correction(filtered)\n",
    "\n",
    "        # average data from the same day\n",
    "        grouped = corrected.groupby(['year', 'month', 'day']).aggregate({'adjusted_data' : np.mean}).reset_index()\n",
    "\n",
    "        # bin the data\n",
    "        final = grouped.groupby(['year', 'month', pd.cut(grouped['adjusted_data'], range(0, 300, 10))]).size().unstack().fillna(0.0)\n",
    "        result.append(final)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
